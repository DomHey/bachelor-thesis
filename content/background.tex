\section{Hintergrund}

Im Rahmen des Bachelorprojektes am Hasso-Plattner-Institut wurde innerhalb eines Jahres die Softwarelösung Noise to Opportunity in einer Gruppe von 8 Personen entwickelt. Ich habe mich mit der Datenakquise beschäftigt. Dazu habe ich eine Vielzahl von Webseiten analysiert um die Registrierungs-, Einlog- und Suchprozesse zu verstehen und programmtechnisch nachzubauen. Bei dieser Tätigkeit ist aufgefallen, dass die meisten Internetforen eine ähnliche Struktur besitzen. Anfänglich war die Herausforderung, sich mit einem händisch angelegten Nutzeraccount im Forum automatisch anzumelden und das Forum zu durchsuchen. Als dieses durch Analyse der Webseiten, sich wiederholende Prozesse erkennen ließ, entstand die Idee, sich auch automatisch registrieren zu wollen.
Viele der Registrierungsformen besitzen gleiche Attributnamen, die sie in dem HTML-Quellcode identifizieren. Der Registrierungsprozess folgt dem gleichen Schema und lässt sich demnach genau wie das Einloggen und das Suchen automatisieren. Es entstand die Idee ein Programm zu entwickeln, das  automatisch Internetinhalte hinter POST- und GET- Formularen indizieren kann.

\subsection{Related Work}

Jianuguo Lu \cite{lu2008efficient} entwickelt in seiner Arbeit eine Formel, die eine Bestimmung der Datenbankgröße ermöglicht, wenn die Datenbank nur über ein Suchanfragen Interface angefragt werden kann. Er beschreibt die Abhängigkeit von der Anzahl der Suchanfragen zur Anzahl der Einträge, die auf eine Suchanfrage zurückgegeben werden. Getestet wird an Datenbanken mit bekannten Größen. Dabei wird eine Konvergenz zur realen Datenbankgröße ab 200 Suchanfragen festgestellt. Die Formel aus dem heterogenen Modell (Seite 1) ist die Grundlage für meine Errechnung der Forendatenbank-Größe, sowie für die Errechnung der Datenbank-Größe für ein spezifisches Firmenprodukt.

Patrick Hennig und Philipp Berger \cite{n2o} untersuchen in ihrer Arbeit die Möglichkeit aus Social-Media-Beiträgen, diejenigen herauszufiltern, die einen Kaufbedarf ausdrücken und sie gleichzeitig einem Produkt zuzuordnen. Dabei untersuchen sie die Richtigkeit verschiedener Algorithmen zum Erkennnen des Kaufbedarfs (Seite 12) sowie Algorithmen zum Klassifizieren des Produktes (Seite 13). Dieser implementierte Service liefert in dieser Arbeit den Ausgangspunkt für die Ermittelung der Datenbankgröße für ein Firmenprodukt.

Juan Ramos \cite{ramos2003using} beschreibt in seiner Arbeit die Relevanz des tf-idf Algorithmus zur Gewinnung von Schlagworten aus Dokumenten. Er zeigt, dass alleinige Gewichtung der Wörter nach Anzahl der Wortvorkommen keine relevanten Schlagworte generieren, sondern erst die Gewichtung von Wortvorkommen im Verhältnis zum Vorkommen in den Dokumenten, diese in seinen Versuchen gut klassifizieren. Dieser Ansatz der tf-idf Wortgenerierung wird eingesetzt, um passende Produktschlagworte aus den Unternehmensunterlagen zu gewinnen. Die Studie belegt, dass diese Worte ein Produkt gut klassifizieren, was an den Evaluationsergebnissen bestätigt werden kann.

Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet \cite{goodfellow2013multi} erläutern in ihrer Arbeit, wie durch den Versuch bei Google-Street-View \footnote{https://www.google.com/maps/views/streetview?gl=de\&hl=de checked: 20.06.2015}, die Hausnummern automatisch zu erkennen, ein Algorithmus entstanden ist, der reCaptchas mit Wahrscheinlichkeit von 99,8\% richtig ausfüllt. Bei der Registrierung in Foren wird zum Teil durch diese reCaptchas validiert, ob sich ein Mensch versucht anzumelden. Soll dieses automatisiert passieren, kann der Algorithmus aus der Arbeit eine gute Grundlage zur Implementierung bieten.

\cite{madhavan2008google} TODO: 

\cite{gupta2014comparative} TODO: 

\cite{jiang2009selectivity} TODO: 

\cite{leopold2002zipfsche} TODO: 
