\section{Hintergrund}

Im Rahmen des Bachelorprojektes am Hasso-Plattner-Institut wurde innerhalb eines Jahres die Softwarelösung `Noise to Opportunity` in einer Gruppe von 8 Personen entwickelt. `Noise to Opportunity` durchsucht soziale Medien und analysiert die gefundenen Beiträge. Drücken diese einen Kaufwunsch aus und können einem Unternehmensprodukt zugeordnet werden, werden diese an Vertreter weitergeleitet, die auf den Verkauf dieses Produktes spezilaisiert sind. Ich habe mich mit der Datenakquise beschäftigt. Dazu habe ich eine Vielzahl von Webseiten analysiert um die Registrierungs-, Einlogg- und Suchprozesse zu verstehen und programmtechnisch nachzubauen. Bei dieser Tätigkeit ist aufgefallen, dass die meisten Internetforen eine ähnliche Struktur besitzen. Anfänglich war die Herausforderung, sich mit einem händisch angelegten Nutzeraccount im Forum automatisch anzumelden und das Forum zu durchsuchen. Die Analyse der Webseiten ließ sich wiederholende Prozesse erkennen, so entstand die Idee, sich auch automatisch registrieren zu wollen.
Viele der Registrierungsformen besitzen gleiche Attributnamen, die sie in dem HTML-Quellcode identifizieren. Der Registrierungsprozess folgt dem gleichen Schema und lässt sich demnach genau wie das Einloggen und das Suchen automatisieren. Es entstand die Idee ein Programm zu entwickeln, das automatisch Internetinhalte hinter POST- und GET- Formularen und des Deep- und Private Webs indizieren könnte. Wir sahen die Möglichkeit, eine Voraussage darüber zu treffen, wie viele Beiträge und wie viele Beiträge zu einem bestimmten Produkt in einem Forum vorhanden sind.
Daraus entwickelte sich die These dieser Arbeit, ob es möglich sei, abzuschätzen wie verkaufsrelevant das Forum für Unternehmensprodukte ist.

\subsection{Related Work}

Jianuguo Lu \cite{lu2008efficient} entwickelt in seiner Arbeit eine Formel, die eine Bestimmung der Datenbankgröße ermöglicht, wenn die Datenbank nur über ein Suchanfragen Interface angefragt werden kann. Er beschreibt die Abhängigkeit von der Anzahl der Suchanfragen zur Anzahl der Einträge, die auf eine Suchanfrage zurückgegeben werden. Getestet wird an Datenbanken mit bekannten Größen. Dabei wird eine Konvergenz zur realen Datenbankgröße ab 200 Suchanfragen festgestellt. Die Formel aus dem heterogenen Modell (Seite 1) ist die Grundlage für die Errechnung der Forendatenbank-Größe, sowie für die Errechnung der Datenbank-Größe für ein spezifisches Firmenprodukt.

Patrick Hennig und Philipp Berger \cite{n2o} untersuchen in ihrer Arbeit die Möglichkeit aus Social-Media-Beiträgen, diejenigen herauszufiltern, die einen Kaufbedarf ausdrücken und diese gleichzeitig einem Produkt zuzuordnen. Dabei untersuchen sie die Richtigkeit verschiedener Algorithmen zum Erkennen des Kaufbedarfs (Seite 12) sowie Algorithmen zum Klassifizieren des Produktes (Seite 13). Dieser implementierte Service liefert in dieser Arbeit den Ausgangspunkt für die Ermittelung der Datenbankgröße für ein Firmenprodukt.

Juan Ramos \cite{ramos2003using} beschreibt in seiner Arbeit die Relevanz des `tf-idf-Algorithmus` zur Gewinnung von Schlagworten aus Dokumenten. Er zeigt, dass die alleinige Gewichtung der Wörter nach Anzahl der Wortvorkommen keine relevanten Schlagworte generiert, sondern erst die Gewichtung von Wortvorkommen im Verhältnis zum Vorkommen in den Dokumenten. Dieser Ansatz der tf-idf Wortgenerierung wird eingesetzt, um passende Produktschlagworte aus den Unternehmensunterlagen zu gewinnen. Die Studie belegt, dass diese Worte ein Produkt gut klassifizieren, was an den Evaluationsergebnissen bestätigt werden kann.

Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet \cite{goodfellow2013multi} erläutern in ihrer Arbeit, wie durch den Versuch bei Google-Street-View \footnote{https://www.google.com/maps/views/streetview?gl=de\&hl=de checked: 20.06.2015}, die Hausnummern automatisch zu erkennen, ein Algorithmus entstand, der `reCaptchas` mit Wahrscheinlichkeit von 99,8\% richtig ausfüllt. Bei der Registrierung in Foren wird zum Teil durch diese `reCaptchas` validiert, ob sich ein Mensch versucht anzumelden. Soll dieses automatisiert passieren, kann der Algorithmus aus der Arbeit eine gute Grundlage zur Implementierung bieten.

In der Arbeit von Edda Leopold \cite{leopold2002zipfsche} wird beschrieben, wie das Zipf'sche Gesetz eine Verteilung der Worthäufigkeiten in einer Sprache generieren kann. Dieses könnte in Kombination mit der Errechnung der Gesamtdatenbankgröße ein besseres Ergebnis erzielen, wenn sich bei einer längeren Evaluation herausstellt, dass die Datenbankgrößen falsch abgeschätzt werden. Sollte das der Fall sein, könnten nur mittels häufig vorkommender Wörter der Sprache gesucht werden, oder die Gewichtung für überlappende Dokumente bei Wörtern aus der unteren oder oberen Zipf Verteilung geändert werden. 

Sonali Gupta, Komal Kumar Bhatia \cite{gupta2014comparative}, sowie Jayant Madhavan u.a \cite{madhavan2008google} beschreiben in ihrer Arbeit die Idee, Teile des Deep Webs zu durchsuchen, indem sie valide Suchanfragen aus dem Suchinterface generieren und an die Webseite absenden. Dazu analysieren sie die Suchformen, füllen sie mit Suchworten und diskutieren die Vor- und Nachteile von breiten- zu tiefenorientierten Crawlern. Sie evaluieren mit wie viel Suchaufwand es möglich ist, einen möglichst großen Anteil der Datenbank zu indexieren. Diese Arbeiten gaben den Anlass dazu, eine Metrik dafür zu entwickeln, welche Formen zum Einloggen und Registrieren vorhanden sind, sowie programmtechnisch die Suchform zu erkennen und anhand der Links als DokumentID die Datenbankgröße zu generieren.

