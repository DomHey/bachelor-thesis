\section{Hintergrund}

Im Rahmen des Bachelorprojektes am Hasso-Plattner-Institut wurde innerhalb eines Jahres die Softwarelösung Noise to Opportunity, in einer Gruppe von 8 Personen, entwickelt. Ich habe mich mit der Datenakquise beschäftigt. Dazu habe ich eine Vielzahl von Webseiten analysiert um die Registrierungs-, Einlog- und Suchprozesse zu verstehen und programmatisch nachzubauen. Bei dieser Tätigkeit ist aufgefallen, dass die meisten Internetforen eine ähnliche Struktur besitzen. Anfänglich war die Herausforderung, sich mit einem händisch angelegten Nutzeraccount, in dem Forum automatisch anzumelden und das Forum zu durchsuchen. Als dieses, durch Analyse der Webseiten, sich wiederholende Prozesse erkennen ließ, entstand die Idee, sich auch automatisch registrieren zu wollen.
Viele der Registrierungsformen besitzen gleiche Attributnamen, die sie in dem HTML-Quellcode identifizieren. Der Registrierungsprozess folgt dem gleichen Schema und lässt sich demnach genau wie das Einloggen und das Suchen automatisieren. Es entstand die Idee ein Programm zu entwickeln, welches automatisch Internetinhalte hinter POST und GET Formularen indizieren kann.

\subsection{Related Work}

Jianuguo Lu \cite{lu2008efficient} entwickelt in seiner Arbeit eine Formel, die eine Bestimmung der Datenbankgröße ermöglicht, wenn die Datenbank nur über ein Suchanfragen Interface angefragt werden kann. Er beschreibt die Abhängigkeit von der Anzahl der Suchanfragen zu Anzahl der Einträge, die auf eine Suchanfrage zurückgegeben werden. Getestet wird an Datenbanken mit bekannten Größen.Dabei wird eine Konvergenz zur realen Datenbankgröße ab 200 Suchanfragen festgestellt. Die Formel aus dem heterogenen Model (Seite 1) stellt die Grundlage für meine Errechnung der Forendatenbank Größe, sowie für die Errechnung der Datenbankgröße für ein spezifisches Firmenprodukt.

Patrick Hennig und Philipp Berger \cite{n2o} untersuchen in ihrer Arbeit die Möglichkeit aus Social Media Posts, diejenigen herauszufiltern, die einen Kaufbedarf ausdrücken und sie gleichzeitig einem Produkt zuzuordnen. Dabei untersuchen sie die Richtigkeit verschiedener Algorithmen zum erkennnen des Kaufbedarfs (Seite 12), sowie Algorithmen zum klassifizieren des Produktes (Seite 13). Dieser implementeierte Service liefert den Ausgangspunkt für die Ermittelung der Datenbankgröße für ein Firmenprodukt in dieser Arbeit.

Juan Ramos \cite{ramos2003using} beschreibt in seiner Arbeit die Relevanz des tf-idf Algorithmus zur Gewinnung von Schlagworten aus Dokumenten. Er zeigt, das alleinige Gewichtung der Worter nach Anzahl der Wortvorkommen  keine relevanten Schlagworte generieren, sondern erst die Gewichtung von Wortvorkommen im Verähltnis zum Vorkommen in den Dokumenten, die Dokumente in seinen Versuchen gut klassifizieren. Dieser Ansatz der tf-idf Wortgenerierung wird eingesetzt um passende Produktschlagworte aus den Unternehmensunterlagen zu gewinnen. Die Studie belegt das diese Worte ein Produkt gut klassifizieren, welches an den Evaluationsergebnissen bestätigt werden kann.

Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet \cite{goodfellow2013multi} erläutern in ihrer Arbeit, wie durch den Versuch bei Google-Street-View \footnote{https://www.google.com/maps/views/streetview?gl=de\&hl=de checked: 20.06.2015} die Hausnummern automatisch zu erkennen, ein Algorithmus entstanden ist, welche den heutigen Standard setzt, um Menschen von Computern zu unterscheiden, mit einer Wahrscheinlichkeit von 99,8\% richtig ausfüllt. Bei der Registrierung in Foren wird zum Teil durch diese reCaptchas validiert ob sich ein Mensch versucht anzumelden. Soll dieses automatisiert passieren kann der Algorithmus aus der Arbeit eine gute Grundlage zur Implementierung bieten.

\cite{madhavan2008google} TODO: 

\cite{gupta2014comparative} TODO: 

\cite{jiang2009selectivity} TODO: 

\cite{leopold2002zipfsche} TODO: 