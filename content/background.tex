\section{Hintergrund}

Im Rahmen des Bachelorprojektes am Hasso-Plattner-Institut wurde innerhalb eines Jahres die Softwarelösung Noise to Opportunity in einer Gruppe von 8 Personen entwickelt. Ich habe mich mit der Datenakquise beschäftigt. Dazu habe ich eine Vielzahl von Webseiten analysiert um die Registrierungs-, Einlog- und Suchprozesse zu verstehen und programmtechnisch nachzubauen. Bei dieser Tätigkeit ist aufgefallen, dass die meisten Internetforen eine ähnliche Struktur besitzen. Anfänglich war die Herausforderung, sich mit einem händisch angelegten Nutzeraccount im Forum automatisch anzumelden und das Forum zu durchsuchen. Als dieses durch Analyse der Webseiten, sich wiederholende Prozesse erkennen ließ, entstand die Idee, sich auch automatisch registrieren zu wollen.
Viele der Registrierungsformen besitzen gleiche Attributnamen, die sie in dem HTML-Quellcode identifizieren. Der Registrierungsprozess folgt dem gleichen Schema und lässt sich demnach genau wie das Einloggen und das Suchen automatisieren. Es entstand die Idee ein Programm zu entwickeln, das automatisch Internetinhalte hinter POST- und GET- Formularen indizieren kann. Wenn es ein Programm gäbe, das automatisch Inhalte des Deep- und Private Webs indizieren könnte, entstand die Idee eine Voraussage darüber zu treffen, wie viele Beiträge in einem Forum vorhanden sind und einhergehend, wie viele Beiträge zu einem bestimmten Produkt in dem Forum vorhanden sind.
Daraus entwickelte sich die These dieser Arbeit, ob es möglich sei, abzuschätzen wie verkaufsrelevant das Forum für Unternehmensprodukte ist.

\subsection{Related Work}

Jianuguo Lu \cite{lu2008efficient} entwickelt in seiner Arbeit eine Formel, die eine Bestimmung der Datenbankgröße ermöglicht, wenn die Datenbank nur über ein Suchanfragen Interface angefragt werden kann. Er beschreibt die Abhängigkeit von der Anzahl der Suchanfragen zur Anzahl der Einträge, die auf eine Suchanfrage zurückgegeben werden. Getestet wird an Datenbanken mit bekannten Größen. Dabei wird eine Konvergenz zur realen Datenbankgröße ab 200 Suchanfragen festgestellt. Die Formel aus dem heterogenen Modell (Seite 1) ist die Grundlage für meine Errechnung der Forendatenbank-Größe, sowie für die Errechnung der Datenbank-Größe für ein spezifisches Firmenprodukt.

Patrick Hennig und Philipp Berger \cite{n2o} untersuchen in ihrer Arbeit die Möglichkeit aus Social-Media-Beiträgen, diejenigen herauszufiltern, die einen Kaufbedarf ausdrücken und sie gleichzeitig einem Produkt zuzuordnen. Dabei untersuchen sie die Richtigkeit verschiedener Algorithmen zum Erkennen des Kaufbedarfs (Seite 12) sowie Algorithmen zum Klassifizieren des Produktes (Seite 13). Dieser implementierte Service liefert in dieser Arbeit den Ausgangspunkt für die Ermittelung der Datenbankgröße für ein Firmenprodukt.

Juan Ramos \cite{ramos2003using} beschreibt in seiner Arbeit die Relevanz des tf-idf Algorithmus zur Gewinnung von Schlagworten aus Dokumenten. Er zeigt, dass alleinige Gewichtung der Wörter nach Anzahl der Wortvorkommen keine relevanten Schlagworte generieren, sondern erst die Gewichtung von Wortvorkommen im Verhältnis zum Vorkommen in den Dokumenten, diese in seinen Versuchen gut klassifizieren. Dieser Ansatz der tf-idf Wortgenerierung wird eingesetzt, um passende Produktschlagworte aus den Unternehmensunterlagen zu gewinnen. Die Studie belegt, dass diese Worte ein Produkt gut klassifizieren, was an den Evaluationsergebnissen bestätigt werden kann.

Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet \cite{goodfellow2013multi} erläutern in ihrer Arbeit, wie durch den Versuch bei Google-Street-View \footnote{https://www.google.com/maps/views/streetview?gl=de\&hl=de checked: 20.06.2015}, die Hausnummern automatisch zu erkennen, ein Algorithmus entstanden ist, der reCaptchas mit Wahrscheinlichkeit von 99,8\% richtig ausfüllt. Bei der Registrierung in Foren wird zum Teil durch diese reCaptchas validiert, ob sich ein Mensch versucht anzumelden. Soll dieses automatisiert passieren, kann der Algorithmus aus der Arbeit eine gute Grundlage zur Implementierung bieten.

In der Arbeit von Edda Leopold \cite{leopold2002zipfsche} wird beschreiben, wie das Zipf'sche Gesetz eine Verteilung der Worthäufigkeiten in einer Sprache generieren kann. Dieses könnte in Kombination mit der Errechnung der Gesamtdatenbankgröße ein besseres Ergebnis erzielen, wenn sich bei einer längeren Evaluation herausstellt, dass die Datenbankgrößen falsch abgeschätzt werden. Sollte das der Fall sein, könnten nur mittel häufig vorkommende Wörter der Sprache gesucht werden, oder die Gewichtung für überlappende Dokumente bei Wörtern aus der unteren oder oberen Zipf Verteilung geändert werden. 

Sonali Gupta, Komal Kumar Bhatia \cite{gupta2014comparative}, sowie Jayant Madhavan u.a \cite{madhavan2008google} beschreiben in ihrer Arbeit, die Idee Teile des Deep Webs zu durchsuchen, indem sie valide Suchanfragen aus dem Suchinterface generieren und an die Webseite absenden. Dazu analysieren sie die Suchformen, füllen sie mit Suchworten und diskutieren die Vor- und Nachteile von breiten- zu tiefenorientierten Crawlern. Sie evaluieren mit wie viel Suchaufwand es möglich ist einen möglichst großen Anteil der Datenbank zu indexieren. Diese Arbeiten gaben den Anlass dazu, eine Metrik dafür zu generieren, welche Formen zum Einloggen und Registrieren vorhanden sind, sowie programmtechnisch die Suchform zu erkennen und anhand der Links als DokumentID die Datenbankgröße zu generieren.

