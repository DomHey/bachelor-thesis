\section{Related work}
Jianuguo Lu \cite{lu2008efficient} developed in his work a formula that allows a determination of the size of the database when the database can be requested via a web interface requests only. He describes the dependence of the number of searches by number of entries that are returned on a search query. He tested his formula on databases of known sizes. A convergence to the real database size with only 200 queries was detected. The formula of the heterogeneous model (page 1) is the basis for calculating the forums database size and for the calculation of the database size to a specific company product.

Patrick Hennig und Philipp Berger \cite{n2o} explore the possibility of extracting posts from social media, who express a demand a purchase and simultaneously classify the post, to which category he belongs. They examine the accuracy of various algorithms for recognizing the demand (page 12) as well as algorithms for classifying the product (page 13). This implemented service is the starting point for estimating the company product specific database size of a forum.

Juan Ramos \cite{ramos2003using} describes in his work the importance of the `tf-idf-algorithm` for generating keywords from documents. He shows that the sole emphasis of words on word count does not generate relevant keywords, but only the weighting of word occurrences in relation to their occurrence in the documents. This approach is used for generating product relevant keywords from company documents. His study shows that these words classify a product well and can be confirmed with the my evaluation results.

Ian J. Goodfellow, Yaroslav Bulatov, Julian Ibarz, Sacha Arnoud, Vinay Shet \cite{goodfellow2013multi} explain in their work, how to correctly solve `reCaptchas` with a probavility of 99,8\%. The introduced algorithm was developed while trying to automatically recognize the house number on google street view~\footnote{https://www.google.com/maps/views/streetview?gl=de\&hl=de checked: 20.06.2015} pictures. When registering in forums `reCaptchas` are used to validate if a human is trying to register. If the `reCaptchas` should be filled in automatically the described algorithm can provide a good basis for implementation.

In the work of Leopold Edda\cite{leopold2002zipfsche} describes how the Zipf's law can generate a distribution of word frequencies in any language.
This could achieve a better database estimation result, if  a long term evaluation shows that the database sizes are estimated incorrectly. If this is the case, it could be searched only by means of common words in a language, or the weighting of overlapping documents with words from the lower or upper Zipf distribution could be changed.

Sonali Gupta, Komal Kumar Bhatia \cite{gupta2014comparative}, as well as Jayant Madhavan et al.\@\cite{madhavan2008google} describe in their work the idea to crawl parts of the Deep Web by generating valid search queries from search interfaces and send them to the website. In addition, they analyze the search forms, fill them with keywords and discuss the advantages and disadvantages of width to depth- crawlers. They evaluate how much effort must be done to index the largest possible part of the database. These works gave the occasion to develop a method to automatically identify forms in websites for registering, login and search, as well as to determine the database size with links to posts as unique ID.

\newpage