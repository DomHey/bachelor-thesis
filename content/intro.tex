%\section {Einleitung}
%Mit der Software `Noise to Opportunity` wurde eine Lösung entwickelt, die es ermöglicht, aus den unmengen von Social Media Posts, aus Linkedin, Facebook, Xing und Twitter diejenigen zu extrahieren, die einen Kaufwunsch an einem bestimmten Produkt äußern. Innerhalb eines Jahres sind ca. 50.000 Posts untersucht worden, von denen %allerdings nur 28.000 Beiträge einen Kaufbedarf ausgedrückt haben. Von diesen 28.000 Beiträgen konnten nur 3.000 mit einer Wahrscheinlichkeit von über 60 \% zu einem zu vertreibenen Produkt zugeordnet werden. Dieses bedeutet, das nur 11 \% der Beiträge überhaupt eine Verkaufschance bieten.
%Es ist zu sehen, dass es zu wenig sinvolle Daten gibt um dieses Produkt in Großfirmen einzusetzen. Dieses lässt sich beheben, indem die \\ Datenaquise in mehr Internetforen oder Portalen fortgesetzt wird.\\
%Diese neuen Datenquellen müssen durchsucht werden. Es muss per Hand ein Crawler für die Webseite geschrieben werden, wenn sie keine API anbietet. Dazu muss die Webseite aufwendig analysiert werden um die ganzen Login- und Suchanfragen in diesem Forum zu verstehen. Ist dieses geschafft, besteht die Gefahr, dass dieses Forum keine %sinnvollen Daten bereitstellt und somit die ganze Zeit, die investiert wurde, vergebens ist.\\
%Genau aus diesem Grund wird hier eine generische Lösung beschrieben und getestet, wie das händische Analysieren und Crawlen einer Webseite automatisiert werden kann. Ziel ist es, einerseits der Webseite ein unnötiges crawlen zu ersparen, wenn keine passenden Daten in dem Forum vorhanden sind und anderseits eine generelle Aussage %über die Relevanz, das heißt Forengröße und am wahrscheinlichst zu verkaufenden Produkt, treffen zu können. Anhand dieser Informationen kann entschieden werden, ob es sich lohnt einen Crawler manuell zugeschnitten auf die Webseite zu erarbeiten oder nicht.\\
%Um dieses automatisieren zu können, muss es programmatisch möglich sein, die Daten im private Web analysieren zu können. Das private Web ist ein Teil des Deep Webs, welches die Suchmaschienen nicht indizieren können, da der Inhalt des private Webs durch Passwort und Login geschützt ist. Da das Deep Web bis zu 2000 mal so groß ist wie %das durch Google durchsuchbare Surface Web \cite{gupta2014comparative}, gehen dort bisher eine Menge der Internetinhalte verloren, die für das Verkaufsgeschäft eine tragende Rolle spielen könnten. Um diese Daten untersuchen zu können müssen die Restriktionen, wie Passwort und Loginabfrage, die das private Web an die %Suchmaschienen stellt, umgangen werden.
%Deshalb beschäftig sich diese Arbeit mit der These, dass es möglich sein sollte, sich automatisch in Internetforen zu registrieren, anzumelden und automatisiert die Datenbankgröße des Forums zu bestimmen, sowie eine Aussage darüber zu treffen, wie geeignet das Forum zum verkaufen eines bestimmten Produktes ist.
%Der erste Teil der Arbeit befasst sich damit, wie automatisch Post- und Get Formulare identifiziert und befüllt werden können. Dieses legt die Grundlage für ein automatisiertes Registrieren, Anmelden und Suchen in den Foren. Der zweite Teil dieser Arbeit befasst sich mit der Evaulation der Theorie, anhand einer Testdatenbank die im Zuge des %Projektes Noise to Opportunity entstanden ist.
%\newpage

\section{Einleitung}
Eine Studie\footnote{http://www.forbes.com/sites/markfidelman/2013/05/19/study-78-of-salespeople-using-social-media-outsell-their-peers/ checked: 29.6.2015} aus dem Jahr 2013 belegt, dass 78,6\% der Verkäufer, die für ihre Verkäufe Social Media benutzen, in ihren Verkauszahlen jene iher Konkurrenten überbieten, die Social Media nicht für ihre Verkäufe heranziehen.
Die Studie bestätigt, dass nicht nur im Alltag, sondern auch im Verkaufsgeschäft Social Media immer mehr an Bedeutung gewinnt, da dort potenzielle neue Kunden akquiriert werden können. Immer häufiger verfasst ein Unternehmen, das eine neue Softwarelösung benötigt, einen Beitrag in einem Social Media Forum. Es erhofft sich davon, dass es ein breites Informationsangebot an passenden Produkten vorgeschlagen bekommt, ohne sich aktiv darum kümmern zu müssen. Der Verkäufer in einem Unternehmen, der eine solche Softwarelösung verkauft, kann, wenn er den Beitrag händisch findet, diesen kommentieren und sein Produkt vorschlagen. 
Dieses manuelle Durchsuchen der Social Media Foren dauert lange. In einer Sekunde werden auf Facebook\footnote{https://www.facebook.com/ checked : 08.07.2015} 41.000 Beiträge und auf Twitter\footnote{https://twitter.com/ checked : 08.07.2015} 4.633 Beiträge veröffentlicht.\footnote{http://blog.qmee.com/wp-content/uploads/2013/07/Qmee-Online-In-60-Seconds2.png checked : 08.07.2015} Diese manuell zu durchsuchen ist unmöglich. Deshalb werden Programme geschrieben, die diese Foren durchsuchen und relevante Beiträge extrahieren. \\ Pro Minute werden 70 neue Domains registriert.\footnote{http://blog.qmee.com/wp-content/uploads/2013/07/Qmee-Online-In-60-Seconds2.png checked : 08.07.2015} Wenn alle 20 Minuten ein neues Forum entsteht, müsste auch alle 20 Minuten ein neues Programm geschrieben werden, das dieses neue Forum durchsuchen kann. Dazu müsste das Forum aufwendig analysiert werden, um die Login- und Suchanfragen in diesem Forum zu verstehen. Wäre dies geschafft, bestünde die Gefahr, dass dieses Forum keine sinnvollen Daten bereitstellt und somit die gesamte Zeit vergeblich investiert worden wäre.\\
Genau aus diesem Grund wird hier eine allgemeine Lösung beschrieben und getestet, wie das händische Analysieren und Durchsuchen eines Forums automatisiert werden kann. Ziel ist es, eine generelle Aussage zu treffen über die Relevanz dieses Forums hinsichtlich Größe und am wahrscheinlichsten zu verkaufendes Produkt. Anhand dieser Informationen kann entschieden werden ob es sich lohnt einen Crawler, manuell zugeschnitten auf die Webseite, zu erarbeiten oder nicht.
Um das zu automatisieren, muss es programmtechnisch möglich sein, die Daten im privaten Web zu analysieren. Das private Web ist ein Teil des Deep Webs, das von Suchmaschienen nicht indiziert werden kann, da der Inhalt des private Webs durch Passwort und Login geschützt ist. Da das Deep Web bis zu 2.000 mal so groß ist wie das durch Google durchsuchbare Surface Web \cite{gupta2014comparative}, gehen dort bisher eine Menge der Internetinhalte verloren, die für das Verkaufsgeschäft eine wichtige Rolle spielen könnten. Um diese Daten untersuchen zu können, müssen die Restriktionen wie Passwort und Loginabfrage, die das private Web an Anwender stellt, programmtechnisch gelöst werden. Dazu muss sich das Programm automatisch registrieren und einloggen. Danach können Forendatenbankgröße sowie Produktdatenbankgröße errechnet werden, indem über das Suchformular des Forums Produktschlagworte gesucht werden. Sollte das Forum einen hohen Anteil an verkaufsträchtigen Beiträgen für ein Firmenprodukt enthalten, kann dieses Forum als sinnvoll für ein Durchsuchen markiert werden.\\
Im Abschnitt `Voraussetzungen` dieser Arbeit werden benötigte Vorbedingungen wie das automatisierte Registrieren und Einloggen beschrieben. Im Kapitel Implementierung wird das Suchen in Foren und das Berechnen der Datenbankgrößen erläutert. Im Kapitel `Evaluation` wird betrachtet, ob die These: `Es ist möglich, automatisch die Relevanz eines Forums für den Verkauf eines Firmenproduktes zu berechnen`, validiert werden kann oder nicht.
\newpage
