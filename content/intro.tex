\section {Einleitung}
Mit der Software `Noise to Opportunity` wurde eineLösung entwickelt, die es ermöglicht, aus den unmengen von Social Media Posts, aus Linkedin, Facebook, Xing und Twitter diejenigen zu extrahieren, die einen Kaufwunsch an einem bestimmten Produkt äußern. Innerhalb eines Jahres sind ca. 50.000 Posts untersucht worden, von denen allerdings nur 28.000 Beiträge einen Kaufbedarf ausgedrückt haben. Von diesen 28.000 Beiträgen konnten nur 3.000 mit einer Wahrscheinlichkeit von über 60 \% zu einem zu vertreibenen Produkt zugeordnet werden. Dieses bedeutet, das nur 11 \% der Beiträge überhaupt eine Verkaufschance bieten.
Es ist zu sehen, dass es zu wenig sinvolle Daten gibt um dieses Produkt in Großfirmen einzusetzen. Dieses lässt sich beheben, indem die \\ Datenaquise in mehr Internetforen oder Portalen fortgesetzt wird.\\
Diese neuen Datenquellen müssen durchsucht werden. Es muss per Hand ein Crawler für die Webseite geschrieben werden, wenn sie keine API anbietet. Dazu muss die Webseite aufwendig analysiert werden um die ganzen Login- und Suchanfragen in diesem Forum zu verstehen. Ist dieses geschafft, besteht die Gefahr, dass dieses Forum keine sinnvollen Daten bereitstellt und somit die ganze Zeit, die investiert wurde, vergebens ist.\\
Genau aus diesem Grund wird hier eine generische Lösung beschreiben und getestet, wie das händische Analysieren und Crawlen einer Webseite automatisiert werden kann. Ziel ist es, einerseits der Webseite ein unnötiges crawlen zu ersparen, wenn keine passenden Daten in dem Forum vorhanden sind und anderseits eine generelle Aussage über die Relevanz, das heißt Forengröße und am wahrscheinlichst zu verkaufenden Produkt, treffen zu können. Anhand dieser Informationen kann entschieden werden, ob es sich lohnt einen Crawler manuell zugeschnitten auf die Webseite zu erarbeiten oder nicht.\\
Um dieses automatisieren zu können, muss es programmatisch möglich sein, die Daten im private Web analysieren zu können. Das private Web ist ein Teil des Deep Webs, welches die Suchmaschienen nicht indizieren können, da der Inhalt des private Webs durch Passwort und Login geschützt ist. Da das Deep Web bis zu 2000 mal so groß ist wie das durch Google durchsuchbare Surface Web \cite{gupta2014comparative}, gehen dort bisher eine Menge der Internetinhalte verloren, die für das Verkaufsgeschäft eine tragende Rolle spielen könnten. Um diese Daten untersuchen zu können müssen die Restriktionen, wie Passwort und Loginabfrage, die das private Web an die Suchmaschienen stellt, umgangen werden.
Deshalb beschäftig sich diese Arbeit mit der These, dass es möglich sein sollte, sich automatisch in Internetforen zu registrieren, anzumelden und automatisiert die Datenbankgröße des Forums zu bestimmen, sowie eine Aussage darüber zu treffen, wie geeignet das Forum zum verkaufen eines bestimmten Produktes ist.
Der erste Teil der Arbeit befasst sich damit, wie automatisch Post- und Get Formulare identifiziert und befüllt werden können. Dieses legt die Grundlage für ein automatisiertes Registrieren, Anmelden und Suchen in den Foren. Der zweite Teil dieser Arbeit befasst sich mit der Evaulation der Theorie, anhand einer Testdatenbank die im Zuge des Projektes Noise to Opportunity entstanden ist.
\newpage